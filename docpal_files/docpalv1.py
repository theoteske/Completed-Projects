import logging
import os
import json
import pathlib
#import requests
import streamlit as st
import torch
from langchain_community.llms import Ollama
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import (
    CSVLoader,
    PyMuPDFLoader,
    TextLoader,
    UnstructuredPowerPointLoader,
    Docx2txtLoader,
    UnstructuredExcelLoader,
)
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceInstructEmbeddings

logger = logging.getLogger(__name__)

FILE_LOADERS = {
    'csv': CSVLoader,
    'docx': Docx2txtLoader,
    'pdf': PyMuPDFLoader,
    'pptx': UnstructuredPowerPointLoader,
    'txt': TextLoader,
    'xlsx': UnstructuredExcelLoader,
}

ACCEPTED_FILE_TYPES = list(FILE_LOADERS)

#message classes
class Message:
    '''
    base message class
    '''
    def __init__(self, content):
        self.content = content


class HumanMessage(Message):
    '''
    represents a message from the user
    '''


class AIMessage(Message):
    '''
    represents a message from the AI
    '''

#load embeddings model
@st.cache_resource
def load_model():
    #check if GPU is available, otherwise use CPU and warn
    if torch.cuda.is_available():
        device = 'cuda'
    else:
        device = 'cpu'
        st.warning('CUDA is not available. Falling back to CPU. This may result in slower performance.')

    #download Instructor XL embeddings model
    with st.spinner(f'Downloading Instructor XL Embeddings Model locally on {device}...please be patient'):
        try:
            embedding_model = HuggingFaceInstructEmbeddings(
                model_name='hkunlp/instructor-large', 
                model_kwargs={'device': device}
            )
            st.success('Model loaded successfully.')
        except Exception as e:
            st.error(f'Failed to load the embedding model: {e}')
            st.stop()
        
    return embedding_model

#main class to handle the interface with the LLM
class ChatWithFile:
    
    #perform initial parsing of uploaded file and initialize chat instance
    def __init__(self, file_path, file_type):
        '''
        :param file_path: full path and name of uploaded file
        :param file_type: file extension determined after upload
        '''
        self.embedding_model = load_model()
        self.vectordb = None

        #load and split the document into pages
        loader = FILE_LOADERS[file_type](file_path=file_path)
        pages = loader.load_and_split()

        #split the pages into smaller chunks called docs
        docs = self.split_into_chunks(pages)

        #store each doc in chroma vector database
        self.store_in_chroma(docs)

        #set up memory so it remembers previous prompts and answers
        self.memory = ConversationBufferMemory(
            memory_key='chat_history',
            return_messages=True
        )

        #set up llama3 LLM
        self.llm = Ollama(model='llama3')

        #create a ConversationalRetrievalChain object
        self.qa = ConversationalRetrievalChain.from_llm(
            self.llm,
            self.vectordb.as_retriever(search_kwargs={'k': 10}),
            memory=self.memory
        )

        #set up empty conversation history
        self.conversation_history = []

    #split the document pages into chunks based on similarity
    def split_into_chunks(self, pages):
        '''
        :param pages: list of document pages
        :return: result of langchain_experimental.text_splitter.SemanticChunker
        '''
        text_splitter = SemanticChunker(
            embeddings=self.embedding_model,
            breakpoint_threshold_type="percentile"
        )
        return text_splitter.split_documents(pages)

    #if the provided doc contains a metadata dict, iterate over the 
    #metadata and ensure values are stored as strings
    def simplify_metadata(self, doc):
        '''
        :param doc: chunked document to process
        :return: document with any metadata values cast to string
        '''
        metadata = getattr(doc, "metadata", None)
        if isinstance(metadata, dict):
            for key, value in metadata.items():
                if isinstance(value, (list, dict)):
                    metadata[key] = str(value)
        return doc

    #process the scoring for each document generated by the LLM
    def reciprocal_rank_fusion(self, all_results):
        '''
        :param all_results: all score results generated by a query
        :return: sorted dict of all document scores
        '''
        fused_scores = {}
        for result in all_results:
            doc_id = result["query"]
            if doc_id not in fused_scores:
                fused_scores[doc_id] = {"doc": result, "score": 0}
            fused_scores[doc_id]["score"] += 1

        reranked_results = sorted(fused_scores.values(), key=lambda x: x["score"], reverse=True)
        return reranked_results

    #create a composite prompt based on the highest scored documents
    def create_synthesis_prompt(self, original_question, all_results):
        '''
        :param original_question: original prompt sent to the LLM
        :param all_results: sorted (by score) results of original prompt
        :return: prompt for a composite score based on original_question
        '''
        sorted_results = sorted(all_results, key=lambda x: x["score"], reverse=True)
        #st.write("Sorted Results", sorted_results)
        prompt = (
            f"Based on the user's original question: '{original_question}', "
            "here are the answers to the original and related questions, "
            "ordered by their relevance (with RRF scores). Please synthesize "
            "a comprehensive answer focusing on answering the original "
            "question using all the information provided below, ensuring "
            "that the answer is not overly verbose and is relevant to the "
            "original question:\n\n"
        )

        for idx, result in enumerate(sorted_results):
            prompt += f"Answer {idx + 1} (Score: {result['score']}): {result['answer']}\n\n"

        prompt += (
            "Given the above answers, especially considering those with "
            "higher scores, please provide the best possible composite answer "
            "to the user's original question."
        )

        return prompt

    #store each document in Chroma
    def store_in_chroma(self, docs):
        '''
        :param docs: result of splitting pages into chunks with split_into_chunks
        '''
        docs = [self.simplify_metadata(doc) for doc in docs]
        self.vectordb = Chroma.from_documents(docs, embedding=self.embedding_model)
        self.vectordb.persist()

    #if a response is received that should have JSON embedded in the
    #output string, look for the opening and closing tags ([]) then extract
    #the matching text
    def extract_json_from_response(self, response_text):
        '''
        :param response_text: response from LLM that might contain JSON
        :return: Python object returned by json.loads. If no JSON response
            was identified, an empty tuple.
        '''
        json_result = ()
        try:
            json_start = response_text.find('[')
            json_end = response_text.rfind(']') + 1
            json_str = response_text[json_start:json_end]
            json_result = json.loads(json_str)
        except (ValueError, json.JSONDecodeError) as e:
            logger.error("Failed to parse JSON: %s", e)
        return json_result

    #create a list of related queries based on the initial question
    def generate_related_queries(self, original_query):
        '''
        :param original_query: original prompt
        :return: related queries generated by the LLM; if none, empty tuple
        '''
        prompt = (
            f"In light of the original inquiry: '{original_query}', let's "
            "delve deeper and broaden our exploration. Please construct a "
            "JSON array containing four distinct but interconnected search "
            "queries. Each query should reinterpret the original prompt's "
            "essence, introducing new dimensions or perspectives to "
            "investigate. Aim for a blend of complexity and specificity in "
            "your rephrasings, ensuring each query unveils different facets "
            "of the original question. This approach is intended to "
            "encapsulate a more comprehensive understanding and generate the "
            "most insightful answers possible. Only respond with the JSON "
            "array itself."
        )
        response = self.llm.invoke(input=prompt)

        if hasattr(response, 'content'):
            generated_text = response.content
        elif isinstance(response, dict):
            generated_text = response.get('content')
        else:
            generated_text = str(response)
            #st.error("Unexpected response format.")

        related_queries = self.extract_json_from_response(generated_text)
        return related_queries

    #main chat interface
    def chat(self, question):
        '''
        :param question: Initial question asked by the uploader
        '''
        #generate a list of queries to send to the LLM
        related_queries_dicts = self.generate_related_queries(question)
        related_queries_list = [q["query"] for q in related_queries_dicts]
        queries = [question] + related_queries_list

        all_results = []

        #collect responses and append to the conversation_history instance 
        #attribute, for display after the chat completes
        for query_text in queries:
            response = self.qa.invoke(query_text)
            if response:
                st.write("Query: ", query_text)
                st.write("Response: ", response["answer"])
                all_results.append(
                    {
                        "query": query_text,
                        "answer": response["answer"]
                    }
                )
            else:
                st.write("No response received for: ", query_text)

        #if there are results, rerank them and synthesize a response
        if all_results:
            reranked_results = self.reciprocal_rank_fusion(all_results)
            scored_results = [{"score": res["score"], **res["doc"]} for res in reranked_results]
            synthesis_prompt = self.create_synthesis_prompt(question, scored_results)
            synthesized_response = self.llm.invoke(synthesis_prompt)

            if synthesized_response:
                st.write(synthesized_response)
                final_answer = synthesized_response
            else:
                final_answer = "Unable to synthesize a response."

            self.conversation_history.append(HumanMessage(content=question))
            self.conversation_history.append(AIMessage(content=final_answer))

            return {"answer": final_answer}

        #if there are no results, say so
        self.conversation_history.append(HumanMessage(content=question))
        self.conversation_history.append(AIMessage(content="No answer available."))
        return {"answer": "No results were available to synthesize a response."}

#present the file upload context and load the file
def upload_and_handle_file():

    #present the file upload context
    st.title("DocPal - Talk to a Document")
    uploaded_file = st.file_uploader(
        label=(
            f"Choose a {', '.join(ACCEPTED_FILE_TYPES[:-1]).upper()}, or "
            f"{ACCEPTED_FILE_TYPES[-1].upper()} file"
        ),
        type=ACCEPTED_FILE_TYPES
    )

    #if valid upload, set session state for the file path and type of file
    #for use in the chat interface
    if uploaded_file:
        file_type = pathlib.Path(uploaded_file.name).suffix
        file_type = file_type.replace(".", "")

        if file_type:
            csv_pdf_txt_path = os.path.join("temp", uploaded_file.name)
            if not os.path.exists("temp"):
                os.makedirs("temp")
            with open(csv_pdf_txt_path, "wb") as f:
                f.write(uploaded_file.getvalue())
            st.session_state["file_path"] = csv_pdf_txt_path
            st.session_state["file_type"] = file_type
            st.success(f"{file_type.upper()} file uploaded successfully.")
            st.button(
                "Proceed to Chat",
                on_click=lambda: st.session_state.update({"page": 2})
                )
        else:
            st.error(
                f"Unsupported file type. Please upload a "
                f"{', '.join(ACCEPTED_FILE_TYPES[:-1]).upper()}, or "
                f"{ACCEPTED_FILE_TYPES[-1].upper()} file."
            )

#main chat interface, invoked after a file has been uploaded
def chat_interface():

    #check if a file has been uploaded
    st.title('DocPal - Talk to a Document')
    file_path = st.session_state.get('file_path')
    file_type = st.session_state.get('file_type')
    if not file_path or not os.path.exists(file_path):
        st.error('File missing. Please go back and upload a file.')
        return

    #set session state for the chat instance
    if 'chat_instance' not in st.session_state:
        st.session_state['chat_instance'] = ChatWithFile(
            file_path=file_path,
            file_type=file_type
        )

    #present the chat interface
    user_input = st.text_input('Ask a question about your document:')
    if user_input and st.button('Send'):
        with st.spinner('Thinking...'):
            top_result = st.session_state['chat_instance'].chat(user_input)

            if top_result:
                st.markdown('**Top Answer:**')
                st.markdown(f"> {top_result['answer']}")
            else:
                st.write('No top result available.')
            
            st.markdown('**Chat History:**')
            for message in st.session_state['chat_instance'].conversation_history:
                prefix = '*You:* ' if isinstance(message, HumanMessage) else '*AI:* '
                st.markdown(f'{prefix}{message.content}')


if __name__ == '__main__':
    if 'page' not in st.session_state:
        st.session_state['page'] = 1

    if st.session_state['page'] == 1:
        upload_and_handle_file()
    elif st.session_state['page'] == 2:
        chat_interface()