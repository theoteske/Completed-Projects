# Completed Projects in Python

1. DocPal - using retrieval augmented generation (RAG), I created a streamlit app that accepts an input document (pdt, docx, csv, txt, xlsx, pptx) and allows the user to "talk" to the document via the Llama3 large language model. In v1, for each user prompt, I ask the LLM to generate multiple related prompts and answer each of them, and then synthesize a final comprehensive answer that summarizes all the useful information related to the prompt. In v2, I adapt the RAPTOR model (https://arxiv.org/html/2401.18059v1), a RAG methodology that creates a tree-organized retrieval method. At the bottom is a leaf layer, containing text chunks returned by the chunker model. We cluster these initial chunks in terms of semantic similarity, then use an LLM to summarize these clusters. From here, we generate new clusters based on these summaries, until we recursively reach a root summary for the document. The v2 of DocPal also automatically summarizes the key points of a document. This was completed using langchain.
2. Uber fare prediction - for this project I focused on thorough and creative feature engineering, taking a very large dataset (200k rows) with only a handful of useful features initially and ending up with more than 80 useful features.
3. Sentiment analysis done three ways - I classify reviews from the IMDb dataset (50k rows) as having either positive or negative sentiment based on three different natural language processing (NLP) techniques. First, I apply a pipeline with a bag-of-words (or n-gram) model using term frequencyâ€“inverse document frequency (tf-idf) weighting followed by a logistic regression classifier. Next, I create a recurrent neural network (RNN) with PyTorch after tokenizing the reviews, employing an embedding layer and an LSTM layer. Finally, I again use PyTorch to fine-tune a pre-trained BERT model on the IMDb dataset.
4. Language modeling with an RNN - Using PyTorch, I train an RNN with an LSTM layer on a .txt file containing the full text of the novel The Mysterious Island by Jules Verne. The model learns the style of the novel and generates text in the same style via autoregressive character-level language modeling.
5. Solving XOR classification with noise - This is a classic problem in machine learning. By creating a custom neural network layer that involves injecting noise into the output, we demonstrate that adding noise is akin to Tikhonov regularization, as discussed by Chris Bishop (https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/bishop-tikhonov-nc-95.pdf). In this manner, we successfully derive a decision boundary that achieves 98% accuracy on the test set.
6. Convolutional neural networks - I begin by implementing a CNN to classify the digits of MNIST. More interestingly, I also perform data augmentation through random transformations of images in the CelebA dataset with the goal of predicting the smile attribute (whether the pictured celebrity is smiling).
7. Sprite race - just for fun, I decided to make obstacle courses for some sprites and have them race through. Sprites are scored based on how few moves it takes them to escape and how well they can avoid colliding with obstacles. You can choose which maze you want to see the sprites race through (there are 3).
